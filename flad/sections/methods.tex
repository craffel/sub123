\section{Adapting MAB algorithms for FLAD}
In this section we describe our variations on the \ex{} and \ucb{} algorithms based on how these algorithms do and do not fit for the FLAD setting. We present pseudo-code for our methods in Algorithms \ref{alg:exp3} and \ref{alg:ucb1}.

% \alon{$t$ has been omitted in algorithm pseudo-code where obvious from context}
\subsection{\ex{} for FLAD}

% Adjustments to \ex{}
Recall that the \ex{} algorithm is designed for the adversarial MAB setting, where no assumptions are made on the reward generation process and in Section~\ref{sec:modelling_assumptions} we argue that the adversarial MAB setting is an appropriate modelling choice for FLAD.
However, in FLAD we can make some assumptions to weaken the power of the adversary and improve the quality of our learner.

% Very bad datasets will likely stay very bad, exploration rate decay
First, we assume that the helpfulness or harmfulness of training on a particular auxiliary dataset (in terms of the impact on target dataset performance) will be relatively consistent throughout training.
% This is a reasonable assumption because if our target gradient is orthogonal or opposite direction of an auxiliary gradient prior to being trained at all, it's likely that their solutions are quite far from each other in parameter space.
Of course, because we randomly sample batches during training, the reward generating process is noisy and we cannot know for sure early on whether a dataset is actually helpful or harmful.
To model this assumption, we use a decaying exploration rate, as proposed by \citet{pmlr-v24-seldin12a}.
% Instead of using a constant exploration rate, $\frac{1}{K}$, we use an exploration rate that changes over time.
Recall that $K$ is the number of auxiliary datasets, then at the beginning of round $t$ let the exploration rate be
\begin{equation}
\label{eq:exploration_rate}
    \mathcal{E}_{t} = \min \Bigl\{\frac{1}{K}, \sqrt{\frac{\ln K}{K\cdot t}} \Bigr\}
\end{equation}
Further, we adjust the sampling distribution from equation \ref{eq:exp3_sampling} to include the new exploration rate as
\begin{equation}
\label{eq:our_exp3_sampling}
    p(a) = (1-K\mathcal{E}_{t})\frac{\exp(\mathcal{E}_{t-1}\hat{R}_{a})}{\sum_{a^{\prime}} \exp(\mathcal{E}_{t-1}R_{a^{\prime}})}+\mathcal{E}_{t}
\end{equation}
Under our assumptions, this decaying rate now allows for an initial exploratory period after which the learner will select ``bad`` actions at a rate of $\mathcal{E}_{t}=\sqrt{\frac{\ln K}{K\cdot t}}$.

% Exp3 Algorithm
\begin{algorithm}[t]
\caption{\ex{}-FLAD}
\label{alg:exp3}
\small
\begin{algorithmic}[1]

% =====================
% Begin ACL version
% =====================

% \Require $\mathcal{D}_{\mathcal{A}},\mathcal{D}_{\mathcal{T}}$: Auxiliary and target datasets
% \Require $\mathit{f}_{\theta}$: Parameterized model
% \Require $G$: Gradient accumulation steps

% \State \textbf{Initialize}: $K=\vert \mathcal{A} \vert$;\quad$\mathcal{E}_{0} = \frac{1}{K}$;
% \Statex \quad$\forall a\in \mathcal{A}: \nabla_{a} = 0$, $\hat{R}_{a} = 1$

% \For{$t=1, 2, \dots, N$}
    
%     \State $\mathcal{E}_{t} = \min \Bigl\{\frac{1}{K}, \sqrt{\frac{\ln K}{K t}} \Bigr\}$
    
%     \State $\forall a\in\mathcal{A}: p(a) \gets (1-K\mathcal{E}_{t})\frac{\exp(\mathcal{E}_{t-1}\hat{R}_{a})}{\sum_{a^{\prime}} \exp(\mathcal{E}_{t-1}R_{a^{\prime}})}+\mathcal{E}_{t}$
    
%     \State Sample dataset $a\sim p(\mathcal{A})$ and batch $\{\mathbf{x},\mathbf{y}\} \sim \mathcal{D}_{a}$
    
%     \State $\nabla_{a}$ $\gets$ $\nabla_{a}+\nabla_{\theta}\mathcal{L}(\mathit{f}_{\theta},\mathbf{x},\mathbf{y})$%\Comment{Update auxiliary dataset gradient}
    
%     \If{$t\ (\mathrm{mod}\ G) \equiv 0$}
        
%         \State $\nabla_{\mathcal{T}}\gets\nabla_{\theta}\mathcal{L}(\mathit{f}_{\theta},\mathcal{D}_{\mathcal{T}})$ %\Comment{Calculate target dataset gradient}
        
%         \State Update model parameters w.r.t.$\nabla_{\mathcal{T}}+\sum_{a}\nabla_{a}$
        
%         \ForAll{$\{ a \in\mathcal{A} \vert \nabla_{a} \neq 0\}$}
        
%             \State $\hat{R}_{a} \gets \hat{R}_{a} + \frac{R_{a,t}}{p(a)}$
            
%             \State $\nabla_{a}\gets 0$ %\Comment{Reset gradients}
%         \EndFor
%     \EndIf
% \EndFor
% =====================
% END ACL / BEGIN ICML
% =====================

\REQUIRE $\mathcal{D}_{\mathcal{A}},\mathcal{D}_{\mathcal{T}}$: Auxiliary and target datasets
\REQUIRE $\mathit{f}_{\theta}$: Parameterized model
\REQUIRE $G$: Gradient accumulation steps

\STATE \textbf{Initialize}: $K=\vert \mathcal{A} \vert$;\quad$\mathcal{E}_{0} = \frac{1}{K}$;
\STATEX \quad$\forall a\in \mathcal{A}: \nabla_{a} = 0$, $\hat{R}_{a} = 1$

\FOR{$t=1, 2, \dots, N$}
    
    \STATE $\mathcal{E}_{t} = \min \Bigl\{\frac{1}{K}, \sqrt{\frac{\ln K}{K \cdot t}} \Bigr\}$ %\COMMENT{Eq. \ref{eq:exploration_rate}}
    
    \STATE $\forall a\in\mathcal{A}: p(a) \gets (1-K\mathcal{E}_{t})\frac{\exp(\mathcal{E}_{t-1}\hat{R}_{a})}{\sum_{a^{\prime}} \exp(\mathcal{E}_{t-1}R_{a^{\prime}})}+\mathcal{E}_{t}$ %\COMMENT{Eq. \ref{eq:our_exp3_sampling}}
    
    \STATE Sample $a\sim p(\mathcal{A})$ and batch $\{\mathbf{x},\mathbf{y}\} \sim \mathcal{D}_{a}$
    
    \STATE $\nabla_{a}$ $\gets$ $\nabla_{a}+\nabla_{\theta}\mathcal{L}(\mathit{f}_{\theta},\mathbf{x},\mathbf{y})$%\Comment{Update auxiliary dataset gradient}
    
    \IF{$t\ (\mathrm{mod}\ G) \equiv 0$}
        
        \STATE $\nabla_{\mathcal{T}}\gets\nabla_{\theta}\mathcal{L}(\mathit{f}_{\theta},\mathcal{D}_{\mathcal{T}})$ %\Comment{Calculate target dataset gradient}
        
        \STATE Update model parameters w.r.t.$\nabla_{\mathcal{T}}+\sum_{a}\nabla_{a}$
        
        \FORALL{$\{ a \in\mathcal{A} \vert \nabla_{a} \neq 0\}$}
        
            \STATE $\hat{R}_{a} \gets \hat{R}_{a} + \frac{R_{a,t}}{p(a)}$
            
            \STATE $\nabla_{a}\gets 0$ %\Comment{Reset gradients}
        \ENDFOR
    \ENDIF
\ENDFOR

% =====================
% END ICML version
% =====================

\end{algorithmic}
\end{algorithm}


% UCB1 Algorithm
\begin{algorithm}[t]
\caption{\ucb{}-FLAD}
\label{alg:ucb1}
\small
\begin{algorithmic}[1]

% =====================
% Begin ACL version
% =====================

% \Require $\mathcal{D}_{\mathcal{A}},\mathcal{D}_{\mathcal{T}}$: Auxiliary and target datasets
% \Require $\mathit{f}_{\theta}$: Parameterized model
% \Require $G$: Gradient accumulation steps
% \Require $\beta$: Smoothing factor

% \State \textbf{Initialize}:
% \Statex $\forall a\in\mathcal{A}: n_{a} = 1$,
% \Statex \qquad\qquad$\hat{R}_{a} = \cos(\nabla_{\theta}\mathcal{L}(\mathit{f}_{\theta},\mathcal{D}_{\mathcal{T}}),\nabla_{\theta}\mathcal{L}(\mathit{f}_{\theta},\mathcal{D}_{a}))$ 

% \For{$t=1, 2, \dots, N$}
    
%     \State $a^{*} = \argmax_{a\in\mathcal{A}}\;\hat{R}_{a} + \sqrt{\frac{2\ln{t}}{n_{a}}}$

%     \State Sample batch $\{\mathbf{x},\mathbf{y}\}\sim \mathcal{D}_{a^{*}}$

%     \State $\nabla_{a^{*}}$ $\gets$ $\nabla_{a^{*}}+\nabla_{\theta}\mathcal{L}(\mathit{f}_{\theta},\mathbf{x},\mathbf{y})$%\Comment{Update auxiliary dataset gradient}

%     \State $n_{a^{*}} \gets n_{a^{*}} + 1$

%     \If{$t\ (\mathrm{mod}\ G) \equiv 0$}

%         \State $\nabla_{\mathcal{T}}\gets\nabla_{\theta}\mathcal{L}(\mathit{f}_{\theta},\mathcal{D}_{\mathcal{T}})$ %\Comment{Calculate target dataset gradient}

%         \State Update model parameters w.r.t. $\nabla_{\mathcal{T}}+\sum_{a}\nabla_{a}$

%         \ForAll{$\{ a \in\mathcal{A} \vert \nabla_{a} \neq 0\}$}

%             \State $\hat{R}_{a} \gets (1-\beta)\hat{R}_{a} + \beta R_{a,t}$

%             \State $\nabla_{a}\gets 0$ %\Comment{Reset gradients}
            
%         \EndFor

%     \EndIf

% \EndFor

% =====================
% END ACL / BEGIN ICML
% =====================

\REQUIRE $\mathcal{D}_{\mathcal{A}},\mathcal{D}_{\mathcal{T}}$: Auxiliary and target datasets
\REQUIRE $\mathit{f}_{\theta}$: Parameterized model
\REQUIRE $G$: Gradient accumulation steps
\REQUIRE $\beta$: Smoothing factor

\STATE \textbf{Initialize}:
\STATEX $\forall a\in\mathcal{A}: n_{a} = 1$,
\STATEX \qquad\qquad$\hat{R}_{a} = \cos(\nabla_{\theta}\mathcal{L}(\mathit{f}_{\theta},\mathcal{D}_{\mathcal{T}}),\nabla_{\theta}\mathcal{L}(\mathit{f}_{\theta},\mathcal{D}_{a}))$ 

\FOR{$t=1, 2, \dots, N$}
    
    \STATE $a^{*} = \argmax_{a\in\mathcal{A}}\;\hat{R}_{a} + \sqrt{\frac{2\ln{t}}{n_{a}}}$

    \STATE Sample batch $\{\mathbf{x},\mathbf{y}\}\sim \mathcal{D}_{a^{*}}$

    \STATE $\nabla_{a^{*}}$ $\gets$ $\nabla_{a^{*}}+\nabla_{\theta}\mathcal{L}(\mathit{f}_{\theta},\mathbf{x},\mathbf{y})$%\Comment{Update auxiliary dataset gradient}

    \STATE $n_{a^{*}} \gets n_{a^{*}} + 1$

    \IF{$t\ (\mathrm{mod}\ G) \equiv 0$}
        \STATE $\nabla_{\mathcal{T}}\gets\nabla_{\theta}\mathcal{L}(\mathit{f}_{\theta},\mathcal{D}_{\mathcal{T}})$ %\Comment{Calculate target dataset gradient}
        \STATE Update model parameters w.r.t. $\nabla_{\mathcal{T}}+\sum_{a}\nabla_{a}$
        \FORALL{$\{ a \in\mathcal{A} \vert \nabla_{a} \neq 0\}$}
            \STATE $\hat{R}_{a} \gets (1-\beta)\hat{R}_{a} + \beta R_{a,t}$
            \STATE $\nabla_{a}\gets 0$ %\Comment{Reset gradients}
        \ENDFOR
    \ENDIF
\ENDFOR

% =====================
% END ICML version
% =====================

\end{algorithmic}
\end{algorithm}

% Allowing for negative reward
In our setting, negative rewards are possible due to the use of cosine similarity to measure gradient alignment in our reward function.
Along with our first assumption (harmful auxiliary data will be harmful throughout training), we believe this leads to reasonable actions in the learner. In the original \ex{} algorithm, the use of an importance-weighted estimated reward compensates the rewards of actions that are less likely to be chosen if their empirical reward is unexpectedly high. In our variation, we allow for a similar phenomenon to occur when an empirical reward is expectedly low. Specifically, if the probability of the selected dataset $a$ is low and the learner receives a negative reward, then our learner was correct in its estimation of a low reward, further decreasing the importance of seeing dataset $a$ in the future. In the original \ex{} the learner can push up on the probability of ``good'' arms, and in our variation we give it the ability to push down on the probability of ``bad'' datasets as well.

% Our adaptation of the Exp3 algorithm
\paragraph{\ex{}-FLAD Algorithm}
% Our variation of \ex{} follows the original \ex{} closely.
On each turn, the learner first computes the current exploration rate as in Equation~\ref{eq:exploration_rate}. Then, the learner samples an auxiliary dataset from the distribution defined by Equation~\ref{eq:our_exp3_sampling}. Next, the learner samples a batch from the chosen auxiliary dataset and calculates the gradient. This process repeats $G$ times, at which point the learner calculates the gradient of the target dataset and updates the model with all auxiliary and target gradients that have accumulated. Finally, the importance-weighted reward for each auxiliary batch is calculated as in Equation~\ref{eq:importance_weighted_reward}, using our cosine reward defined in equation \ref{eq:reward}. See Algorithm~\ref{alg:exp3} for pseudo-code.
% computational complexity
% space complexity

\subsection{\ucb{} for FLAD}
While the \ex{} algorithm accounts for the stochastic, non-stationary reward generating process in FLAD, \ucb{} does not. To address both of these in \ucb{}-FLAD, we include an exponential moving average when estimating the mean reward for a given arm.
Formally, if at turn $t$ the learner selects auxiliary dataset $\mathcal{D}_a$, then we update the estimated mean reward $\hat{R}_{a}$ as 
\begin{equation}
\label{eq:ema_smoothing}
    \hat{R}_{a} = (1-\beta)\hat{R}_{a} + \beta R_{a,t}
\end{equation}
where $\beta$ is the smoothing factor and $R_{a,t}$ is the observed reward.

Furthermore, in the original MAB setting all interactions with the environment occur online, but FLAD is a unique situation where the learner can interact with the auxiliary data prior to training.
To take advantage of this, rather than initializing estimated rewards with a single mini-batch, we propose to initialize them with larger data quantities to improve the approximation of the true dataset gradients (line 1 of Algorithm~\ref{alg:ucb1}).
This is done for each auxiliary dataset by calculating the gradient $\nabla_{a}=\nabla_{\theta}\mathcal{L}(\mathit{f}_{\theta},\mathbf{x},\mathbf{y})$, where the number of samples in $\{\mathbf{x},\mathbf{y}\}$ is significantly larger than a mini-batch, and can be up to the size of the full dataset.

% Our adapation of the UCB1 algorithm
\paragraph{\ucb{}-FLAD Algorithm}
% Our variation on the \ucb{} algorithm mostly follows the original.
First, initialize the estimated rewards $\hat{R}_{a}$ by approximating the gradient from each dataset as described in the previous paragraph. Then at each turn the learner computes the upper confidence bounds for each auxiliary dataset as in Equation~\ref{eq:ucb}. Next, the learner samples a batch of data from the dataset with highest upper confidence bound and calculates the gradient.
% If there is a tie, the learner selects randomly from those that are tied.
This process repeats $G$ times, at which point the learner calculates the gradient of the target dataset and updates the model with all auxiliary and target gradients that have accumulated. Finally, the smoothed estimated mean reward is calculated using Equation~\ref{eq:ema_smoothing}. See Algorithm~\ref{alg:ucb1} for pseudo-code.

% Non-stationary MAB
% Methods for non-stationary MAB are generally based on the UCB1 algorithm, including Discounted UCB \citep{kocsis2006discounted}, Sliding-Window UCB \citep{garivier2008upper}, and Adjusted UCB \citep{bouneffouf2016multi}.

% computational complexity
% space complexity