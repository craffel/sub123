\section{Background}
\alon{Work in Progress}
In this section we first informally describe the FLAD setting, it's challenges, and describe how prior works fall short of our fulfilling our desiderata. Next, we present the multi-armed bandit problem (MAB) and define it's goals in the abstract. Then, we present the adversarial bandit setting, a broadly applicable instantiation of MAB similar to FLAD and the \ex{} algorithm used to solve it. Finally, we present the \ucb{} algorithm, used to solve a more constrained MAB setting.

% Intro to FLAD, which version of FLAD are we focused on
\subsection{Few-shot Learning with Auxiliary Data}
% FLAD is a broad category
Few-shot Learning with Auxiliary Data (\textbf{FLAD}) is a straightforward problem setting, aimed at improving few-shot generalization, and has a very broad set of approaches. FLAD assumes access to a small quantity of target data, $\mathcal{D}_{\mathcal{T}}$, and access to a much larger quantity of (possibly) related auxiliary data, $\mathcal{D}_{\mathcal{A}}$. The auxiliary data may be labeled or unlabeled, be cleanly differentiated as separate datasets/tasks, or have many other properties. The goal of FLAD is to optimize a model for $\mathcal{D}_{\mathcal{T}}$.

% Our assumptions on FLAD
FLAD is a problem setting with a very broad set of candidate solutions. To narrow down the set of solutions that we consider in satisfying our desiderata, we will make some assumptions.
% Target-task aware training
First, we believe that to improve generalization on a specific target dataset, the auxiliary data must be utilized with the target dataset in mind. This removes the consideration of works that set up training into a primary target dataset-agnostic pretraining and a secondary target-only training such as meta-learning \citep{bansal-etal-2020-self} and some multi-task learning methods \citep{aghajanyan-etal-2021-muppet, sanh2022multitask}. These methods lead to models that are capable of generalizing to new or unseen tasks, but are not optimal when the target dataset is known prior to training.
% Possibly many auxiliary tasks
Next, our desiderata make no assumptions on the number of auxiliary datasets. This removes methods such as STILTS \citep{phang2018sentence} and single-source transfer learning \citep{feta_albalak}.
% Don't have unsupervised target data, not always possible
Finally, we assume we have access to a small quantity of labeled target data as it is not always possible to collect thousands of unlabeled target data, as is done in methods such as ReCross \citep{Lin2022UnsupervisedCG} and DEFT \citep{Ivison2022DEFT}.

% Prior works in the broad category
% Prior works on FLAD have set up training in stages, such as in STILTS \citep{phang2018sentence}, transfer learning \citep{feta_albalak}, meta-learning \citep{bansal-etal-2020-self}, or multi-task learning \citep{aghajanyan-etal-2021-muppet, sanh2022multitask}, where the first stage generally uses all of the auxiliary data without consideration of the target data, and the second stage includes training only on the target data. Other approaches have utilized retrieval-augmented models which match auxiliary data to large quantities of unlabeled target data such as in ReCross \citep{Lin2022UnsupervisedCG} and DEFT \citep{Ivison2022DEFT}. Yet other approaches learn to scale the loss of each auxiliary dataset, as in \citep{NEURIPS2019_0e900ad8,Verboven2022}, or include unsupervised auxiliary data as in \citet{deryAANG2022}.

% Prior works more similar to our setting
For this work we focus on the FLAD setting where the final stage of training includes simultaneous training on supervised auxiliary and target data. Prior works in this setting focus on using a single auxiliary task similar to a transfer learning setting \cite{https://doi.org/10.48550/arxiv.1812.02224,chen2022weighted}, or assume prior knowledge on a limited set of available auxiliary tasks \citep{Guo2018DynamicMM}, thereby including an implicit bias in the methodology. In this work we assume access to tens, or even hundreds, of auxiliary datasets and no prior knowledge on their task or domain distributions relative to the target task. 




% Challenges of our FLAD setting
The FLAD setting that we focus on provides some very specific challenges. Our FLAD assumptions on the distribution of auxiliary datasets, an algorithm must be very flexible and general.

To solve such a flexible decision making problem, we draw inspiration from multi-armed bandits, a classic reinforcement learning problem that exemplifies the decision 

% Brief intro to MAB
\subsection{Multi-Armed Bandits}

The Multi-Armed Bandit (\textbf{MAB}) setting is a decision-making problem from machine learning where a learner interacts with an environment over $N$ rounds. At each round, $t$, the learner chooses one of the environment's $K$ arms, $a\in\mathcal{A}$ where $K=\vert\mathcal{A}\vert$, after which the environment provides a reward, $R_{t}$. The rewards for the unplayed arms are not observed. The goal of the learner is to adopt a policy which chooses actions that lead to the largest possible cumulative reward over $N$ rounds, $R=\sum_{t=1}^{N}R_{t}$.
% To evaluate a policy, regret is calculated as the difference between the total expected reward of an optimal policy and the total expected reward collected by the learner \alon{either don't mention regret if not used elsewhere, or include more information on regret}.
In this work we assume a finite $K$ and depending on the exact scenario the underlying reward distribution of each arm may have a variety of properties (eg. stochasticity or stationarity), leading to differing optimal policies and regret bounds \citep{lattimore_szepesvári_2020}.

% Adversarial MAB and Exp3
\paragraph{Adversarial MAB}
% Adversarial MAB details
In this section we discuss the adversarial MAB setting, which makes almost no assumptions on the reward generating process. We will later show why our FLAD formulation fits into the adversarial setting.
% This is opposed to the previously discussed setting for the \ucb{} algorithm, which makes an assumption that rewards are gaussian random variables, and the mean reward of individual arms are (mostly) stationary.

Under this setting, it is assumed that an adversary is given access to the learner's policy and determines the sequence of rewards, $(R_{a,t})_{t=1}^{N}$, for each arm prior to play \citep{auer1995gambling}. At each turn the learner chooses a distribution over actions, $p(\mathcal{A})$, and samples an action from the distribution, $a\sim p(\mathcal{A})$. Policies for the adversarial setting utilize stochasticity to prevent the adversary from knowing which arms will be played.

% Background on EXP3
\paragraph{The \ex{} Algorithm}
The \ex{} algorithm (``Exponential-weight algorithm for Exploration and Exploitation'') is an example of an algorithm for the adversarial multi-armed bandit problem \citet{auer2002nonstochastic}. \ex{} chooses arms according to a Gibbs distribution based on the empirically determined importance-weighted rewards of arms. To allow for exploration, \ex{} mixes the Gibbs distribution with a uniform distribution.

% Importance weighting and Gibbs distribution formulation
Formally, let the exploration rate be $\gamma \in (0,1]$, then at round $t$ define the probability of selecting a given arm, $a\in\mathcal{A}$, as the following linear combination of Gibbs and uniform distributions:
\begin{equation}
\label{eq:exp3_sampling}
    p_{t}(a) = (1-\gamma)\dfrac{e^{\gamma\hat{R}_{a,t-1}/K}}{\sum_{a\prime}e^{\gamma \hat{R}_{a\prime,t-1}/K}}+\frac{\gamma}{K}
\end{equation}
where the importance weighted reward $\hat{R}_{a,t}$ is calculated as 
\begin{equation}
\label{eq:importance_weighted_reward}
    \hat{R}_{a,t} = \hat{R}_{a,t-1} + \frac{R_{a,t}}{p_{t-1}(a)}
\end{equation}
and $R_{a,t}$ is the observed reward. All unplayed arms, $a\prime\neq a$ in round $t$ have unchanged importance weighted rewards; $\hat{R}_{a\prime,t}=\hat{R}_{a\prime,t-1}$.

% Algorithm
Algorithmically, \ex{} is very simple and has three steps at each round. First, calculate the sampling distribution, $p_{t}$, and sample an arm from the distribution. Then a reward, $R_{a,t}$, is observed and the algorithm updates the importance weighted reward, $\hat{R}_{a,t}$, for the played arm.

Informally, the use of an importance-weighted estimated reward compensates the rewards of actions which are less likely to be chosen, guaranteeing that the expected estimated reward is equal to the actual reward for each action. \ex{} is designed to be nearly optimal in the worst case, but due to the exploration rate it will select ``bad'' actions at a rate of $\gamma / K$. The exploration of \ex{} combined with importance-weighting allows the policy to handle non-stationary reward generating processes.

% UCB1 algorithm, regret, optimality, etc.
\paragraph{The \ucb{} Algorithm}
% Background on UCB
While the adversarial setting makes almost no assumptions on the reward generating process and therefore maintains it's performance guarantees under almost any circumstances, it can be outperformed in settings that \textit{are} constrained. In this section we will assume that the reward generating processes are stationary, Gaussian distributions.
% While we will later see that this is not a guarantee in the FLAD setting, we will show that weakened versions of these assumptions hold.
A common policy used to solve the stochastic, (mostly) stationary MAB problem is the Upper Confidence Bound (\ucb{}) algorithm, which assigns each arm a value called the upper confidence bound based on Hoeffding's inequality \citep{auer2002finite}. The \ucb{} algorithm is based on the principle of \textit{optimism in the face of uncertainty}, meaning that with high probability the upper confidence bound assigned to each arm is an overestimate of the unknown mean reward.

% Upper confidence bound formulation
Formally, let the estimated mean reward of arm $a$ after being played $n_{a}$ times be $\hat{R}_{a}$ and the true mean reward be $R_{a}$, then
\[
\mathbb{P}\bigg(R_{a} \ge \hat{R}_{a} + \sqrt{\frac{2\ln (1/\delta)}{n_{a}}}\bigg) \le\delta \quad\forall \delta\in (0,1)
\]
derived from Hoeffding's inequality in equation 7.1 of \citet{lattimore_szepesvári_2020}, where $\delta$ is the confidence level that quantifies the degree of certainty in the arm. In this work, we let $\delta = 1/t$, where $t$ is the number of rounds played, shrinking the confidence bound over rounds. Thus, define the upper confidence bound for arm $a$ at turn $t$ as
\begin{equation}
\label{eq:ucb}
UCB_{a,t} = 
\begin{cases}
\infty,& \text{if } n_{a}=0 \\
\hat{R}_{a}+\sqrt{\frac{2\ln t}{n_{a}}},& \text{otherwise}
\end{cases}
\end{equation}

% Algorithm
Algorithmically, \ucb{} is very simple and has two steps at each round. First, play the arm with largest upper confidence bound, $a^{*}=\arg\max_{a\in\mathcal{A}}UCB_{a,t}$. Next a reward, $R_{a^{*},t}$, is observed and the algorithm updates the estimated mean reward for $a^{*}$, $\hat{R}_{a^{*}}$, and updates the upper confidence bounds for all $a$. Informally, this algorithm suggests that the learner should play arms more often if they either (1) have large expected reward, $\hat{R}$, or (2) $n_{a}$ is small because the arm is not well explored.

% Regret
% The \ucb{} algorithm has a worst-case regret of $O(\sqrt{K n \ln n})$.

% Weighting method
% Given a target dataset $\mathcal{D}_{\mathcal{T}}$, with limited samples, $K$ labeled auxiliary datasets, $\mathcal{D}_{\mathcal{T}}$, and a model $\mathit{f}_{\theta}$, the goal of FLAD is to train $\mathit{f}$ to achieve high performance on the target dataset by simultaneously minimizing the losses
% \[
% \mathcal{L}_{\mathcal{A}\cup\mathcal{T}} = w_{\mathcal{T}}\mathcal{L}_{\mathcal{T}} + \sum_{a\in\mathcal{A}}w_{a}\mathcal{L}_{a}
% \]
% where the weights, $w_{\mathcal{T}},w_{\mathcal{a}}$, are determined by a policy. The policy can be fixed prior to training, adjusted on a predetermined schedule, or learned in an online fashion. In this definition we make no assumptions about the quality of auxiliary data with regards to transfer to the target dataset.
% \alon{Maybe this entire subsection isn't required? State FLAD in it's simplest terms and then move on!}
% Describe multi-task learning and meta-learning under this framework?