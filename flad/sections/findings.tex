\section{Findings and Analyses}

% With T5-Base model
% \begin{table*}[t]
% \caption{Averaged results of FLAD-based methods on T0 evaluation set, reported as mean and standard deviation.}
% \label{tab:main_result}
% \vskip 0.15in
% \begin{center}
% % \begin{small}
% \begin{sc}
% \begin{tabular}{ l | c | c | c | c | c | c }
%     \toprule
%      \multicolumn{1}{r|}{Model} & \multicolumn{2}{c|}{T5-Base} & \multicolumn{2}{c|}{T5-XL} & \multicolumn{2}{c}{T0-3B} \\
%      \multicolumn{1}{r|}{Auxiliary Data} & T0Mix & P3 & T0Mix & P3 & T0Mix & P3\\         
%      \midrule
%     Direct Fine-Tuning &\multicolumn{2}{c|}{45.88\textsubscript{4.30}} & \multicolumn{2}{c|}{52.82\textsubscript{3.34}} & \multicolumn{2}{c}{56.44\textsubscript{4.70}} \\
%     % \hline
%     Exploration-only & 46.93\textsubscript{3.14}&45.46\textsubscript{5.29} & 59.18\textsubscript{5.52} & 60.64\textsubscript{4.92} & 61.17\textsubscript{3.30} & 62.77\textsubscript{4.83} \\
%     % \hline
%     Exploitation-only & 47.16\textsubscript{3.15} & 46.11\textsubscript{4.27} & 59.79\textsubscript{5.63} & 60.49\textsubscript{5.01} & 60.87\textsubscript{3.35} & 62.87\textsubscript{3.69} \\
%     % \hline
%     \ex{}-FLAD & 48.21\textsubscript{4.55} & 47.44\textsubscript{4.37} & 61.50\textsubscript{4.25} & 64.07\textsubscript{4.81} & 62.87\textsubscript{3.85} & \underline{65.98}\textsubscript{3.20} \\
%     % \hline
%     \ucb{}-FLAD & 48.26\textsubscript{3.53} & 48.41\textsubscript{3.67} & 62.01\textsubscript{3.89} & \underline{65.52}\textsubscript{3.86} & 62.89\textsubscript{3.68} & \textbf{66.29}\textsubscript{3.29} \\
%     \bottomrule
% \end{tabular}
% \end{sc}
% % \end{small}
% \end{center}
% \vskip -0.1in
% \end{table*}

% Only 3B models
\begin{table}[t]
\caption{Results of target-only fine-tuning and FLAD-based methods on our target datasets (i.e.\ the held out datasets from T0). Reported scores are mean accuracy and standard deviation over the full evaluation set.}
\label{tab:main_result}
\vskip 0.15in
\begin{center}
\begin{small}
% \begin{sc}
\begin{tabular}{ l | c | c | c | c }
    \toprule
     \multicolumn{1}{r|}{\textsc{Base Model}} & \multicolumn{2}{c|}{\textsc{T5-XL}} & \multicolumn{2}{c}{\textsc{T0-3B}} \\
     \multicolumn{1}{r|}{\textsc{Aux. Data}} & \textsc{T0Mix} & \textsc{P3} & \textsc{T0Mix} & \textsc{P3}
     \\
     \midrule
    Target-Only & \multicolumn{2}{c|}{52.82\textsubscript{3.34}} & \multicolumn{2}{c}{56.44\textsubscript{4.70}} \\
    % \hline
    Explore-Only & 59.18\textsubscript{5.52} & 60.64\textsubscript{4.92} & 61.17\textsubscript{3.30} & 62.77\textsubscript{4.83} \\
    % \hline
    Exploit-Only & 59.79\textsubscript{5.63} & 60.49\textsubscript{5.01} & 60.87\textsubscript{3.35} & 62.87\textsubscript{3.69} \\
    % \hline
    \ex{}-FLAD & 61.50\textsubscript{4.25} & 64.07\textsubscript{4.81} & 62.87\textsubscript{3.85} & \underline{65.98}\textsubscript{3.20} \\
    % \hline
    \ucb{}-FLAD & 62.01\textsubscript{3.89} & \underline{65.52}\textsubscript{3.86} & 62.89\textsubscript{3.68} & \textbf{66.29}\textsubscript{3.29} \\
    \bottomrule
\end{tabular}
% \end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\columnwidth]{images/UCB1_dynamics.png}
%     \vspace{-5mm}
%     \caption{\textbf{Training Dynamics of UCB1}, a case study using WSC as target dataset and T0Mix as auxiliary data. Colored lines represent a sample of auxiliary datasets with qualitatively interesting behavior; the remaining datasets are shown in grey. Shown on top, middle, and bottom are the calculated upper confidence index, the gradient similarity, and the percent of total data that has been sampled from each dataset respectively.}
%     \label{fig:ucb1_dynamics}
% \end{figure}

Now that we have designed and described the \ex{}-FLAD and \ucb{}-FLAD algorithms, we first compare their performance with other FLAD methods. Aggregate scores are shown in Table~\ref{tab:main_result} with detailed results in Table~\ref{tab:detailed_results} of the Appendix. Then, we compare our methods with strong few-shot methods and show the full results in Figure~\ref{fig:comparison}.
% Below may be unnecessary
% We discuss these results in detail in the following paragraphs.


% \paragraph{The Power of FLAD}
\paragraph{FLAD Helps Models Generalize}
First, we compare our proposed methods with target-only fine-tuning (i.e.\ without using auxiliary data) as well as two baseline FLAD settings (explore-only and exploit-only). \textit{Explore-only} is equivalent to multi-task training where auxiliary data is uniformly sampled, i.e.\ continuously exploring auxiliary data and never exploiting knowledge of its relation to the target data. \textit{Exploit-only} computes gradient alignment prior to training, as in \ucb{}, followed by multi-task training where the sampling probabilities are defined by a Gibbs distribution over similarities (similar to that in \ex{}) which results in always exploiting and never exploring. For both FLAD baselines we utilize mixed-dataset batches and a target dataset mixing ratio with the possible values of $\{1,5,10\}$ times the greatest auxiliary sampling probability.

Table \ref{tab:main_result} shows that \textit{all} FLAD methods provide significant improvement in few-shot generalization over target-only fine-tuning, proving the utility of FLAD methods. We find that even the naive mixing approach, \textit{explore-only}, improves performance by 4.8 - 8\%, with the remaining approaches improving accuracy by up to 12.7\%.
Furthermore we find that, given identical training data, all single-stage FLAD methods on T5-XL + T0Mix (left column of Table~\ref{tab:main_result}) improve over the multi-task trained T0 with target-only fine-tuning (between 2.7 - 5.6\% improvement), demonstrating how simultaneous training on auxiliary and target data, as in FLAD methods, can help models generalize in few-shot settings.

\paragraph{The Importance of Exploration \textit{and} Exploitation}
% Although naive FLAD methods improve over target-only training, the improvements quickly taper off as more auxiliary data is added.
% Exploration and exploitation together leads to continued performance improvement.
Our experiments show that \ex{}-FLAD and \ucb{}-FLAD consistently outperform the explore-only and exploit-only FLAD methods across both models and auxiliary datasets. Additionally, Table~\ref{tab:detailed_results} of the appendix shows that our methods surpass the baselines in \textit{every} evaluation task.

Furthermore, when comparing the ability of FLAD methods to leverage additional auxiliary data (i.e.\ going from T0Mix to all of P3), we find that the improvement for explore- and exploit-only methods is minimal with only 0.7-2\% improvement. On the other hand, \ex{}-FLAD and \ucb{}-FLAD show a notable improvement of 2.6-3.5\%, emphasizing the importance of both exploration \textit{and} exploitation, particularly when dealing with large collections of auxiliary data.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{images/sampling_frequency.pdf}
    \vspace{-2mm}
    \caption{\textbf{Empirical Sampling Distributions of FLAD Methods}. A case study on RTE as the target dataset and T0Mixture as auxiliary data.
    % , where \ex{}-FLAD (top) and \ucb{}-FLAD (bottom) outperform baselines.
    Top-4 most sampled auxiliary datasets are highlighted with color. \ucb{}-FLAD forms a distribution with two clear peaks, while \ex{}-FLAD forms a more flat distribution.}
    \label{fig:sampling_frequency}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=0.95\textwidth]{images/flad_comparison.pdf}
    \vspace{-2mm}
    \caption{\textbf{Comparison of FLAD methods trained on P3 with previous few-shot methods}. We calculate all T-Few scores on our data splits using code from~\citet{liu2020tfew}. DEFT-Few scores are taken from~\citet{Ivison2022DEFT}. GPT-3 scores are taken from~\citet{NEURIPS2020_1457c0d6} and utilize few-shot in-context learning. All models utilize the same number of few-shot examples and (other than GPT-3) have 3B parameters.}
    \label{fig:comparison}
\end{figure*}

\paragraph{Comparing \ex{}-FLAD and \ucb{}-FLAD}
In Section~\ref{sec:modelling_assumptions} we argue that adversarial MAB is an appropriate model of the FLAD setting, but our results empirically show that \ex{}-FLAD is slightly outperformed by \ucb{}-FLAD. In this section we analyze each method's performance and training dynamics to understand this phenomenon.

To better understand the training dynamics, we perform a case study on T5-XL with T0Mix as the auxiliary data, with full details and figures in Appendix~\ref{sec:training_dynamics}.
% UCB1 gives a peaky empirical sampling distribution
First we look at RTE, where \ucb{}-FLAD outperforms \ex{}-FLAD. We calculate the empirical distribution of samples seen from each auxiliary dataset, shown in Figure~\ref{fig:sampling_frequency}, and find that \ex{}-FLAD samples nearly uniformly from all datasets while \ucb{}-FLAD forms a bimodal sampling distribution with 2 very clear peaks.
The lack of peakiness in the \ex{}-FLAD distribution is counterintuitive, as we do find that it achieves separation between auxiliary tasks in the cumulative estimated reward (as shown in Figure~\ref{fig:ex_rte}), but this does not lead to separation in the sampling probability space. 
Additionally we find that even on COPA, where \ex{}-FLAD outperforms \ucb{}-FLAD, \ex{}-FLAD still achieves good separation between cumulative estimated rewards, but has a monomodal sampling distribution, while \ucb{}-FLAD does not have as clear of a bimodal distribution as in RTE.

% What causes this peakiness?
The difference in empirical sampling distributions is likely due to the difference between the greedy policy of \ucb{}-FLAD and the stochastic policy of \ex{}-fLAD. Empirically, we find that \ex{}-FLAD very rarely assigns an auxiliary dataset a probability $<1$\%, leading to many ``bad'' batches over the course of thousands of turns. On the other hand, the optimistic policy of \ucb{}-FLAD spends much less time exploring and will sample ``bad'' batches much less frequently.

% While we find that Exp3 is able to very quickly adjust the expected rewards for a dataset, it turns out this is overruled by the non-zero sampling probability it assigns to ``bad'' datasets.
% EXP3 assigns too much weight to exploring (non-zero probability even for really low cumulative reward), and never separates out the good datasets, partly exaggerated by the fact that it never removes datasets from consideration. Turns out our setting does not have adversarial changes in "best" auxiliary datasets. 

\paragraph{Empirical Findings on Gradient Alignment as a Reward}
We find gradient alignment to be a very noisy signal (see Appendix~\ref{sec:training_dynamics}). For \ex{}-FLAD, this may be harm the formation of a policy because the adversary is frequently changing which auxiliary dataset is best, forcing it to make large changes to the sampling distribution. On the other hand, \ucb{}-FLAD smooths the reward signal with an exponential moving average, leading to the model sampling multiple batches from an auxiliary dataset before adjusting the confidence bounds. By sampling multiple batches, \ucb{}-FLAD gets a better approximation for the auxiliary dataset reward.

Furthermore, we find that the rewards are mostly stationary (although highly stochastic) after an initial break-in period of $\sim$100 gradient updates, tending towards 0.
% NOTE: Gradients tending towards 0 may mean that our assumption (negative rewards for EXP3 is okay) is actually wrong, and is hurting the models performance. Unclear whether that is the case though

\paragraph{How can \ex{}-FLAD and \ucb{}-FLAD be improved?}
It is clear that FLAD is not a very strong adversary and that gradient alignments maintain a weak form of stationarity. Taking these findings into account, future methods can utilize this information to improve. First, we show that although gradient alignments tend towards 0, there is useful information in the noisy signal, as proven empirically, which models must try to utilize. Next, we believe that smoothing the reward function can significantly help future methods to make confident decisions. \alon{Is this section meaningful? maybe save the space for related works or discussion instead?} % Not super meaningful but not a waste - may be better for the conclusion


% How much do the specific dataset orderings vary across seeds? Fairly stable, use WSC as example, amazon_polarity is ALWAYS top and next few change exact ordering, but generally are top
% Similar analysis for Exp3, cumulative estimated reward for most similar datasets

% (6) Opposite trend for base vs. large models for increasing K. Base models get worse overall when switching from T0Mixture to P3. Large models get better. \alon{Only if including experiments with base-sized model}


% \paragraph{Training Dynamics of \ucb{}-FLAD}
% \alon{We do a small-scale analysis on the training dynamics of \ucb{}-FLAD to better understand it's performance, and how to improve on this algorithm in the future. Gradients jump from negative to positive.}
% (4) Analysis of learner (how do similarities change over time, compared with static policy)
% Interesting analysis would be to look at the distribution of batch similarities, see just how noisy our reward signal is (spoiler, VERY NOISY)
% for Exp3, look at instantaneous sampling distribution (defined at each time step), or at cumulative estimated (cumulative importance weighted) reward to see what exp3 believes it's reward has been
% for ucb1 look at upper confidence index and empirical dataset proportion
% To make figures more readable, we only show auxiliary datasets which were in the top/bottom k at the beginning and end of training

\paragraph{FLAD Provides Robustness Compared with Non-FLAD Methods}
In this section, we compare the performance of our methods trained on P3 with competitive few-shot methods: T-Few, DEFT-Few, and GPT-3.
T-Few~\citep{liu2020tfew} is a variant of the T0-3B model that multi-task pre-trains parameter-efficient IA$^{3}$ modules followed by target-only fine-tuning the (IA)$^{3}$ modules.
DEFT-Few~\citep{Ivison2022DEFT} is a variant of the T5-XL model that has been multi-task trained using retrieved auxiliary data similar to the target dataset. Using 1000 unlabeled target dataset samples, DEFT first multi-task trains a T5-XL model on the 500 nearest neighbor samples from P3. DEFT then involves target-only fine-tuning with the (IA)$^{3}$ modules from~\citet{liu2020tfew}.
Finally, we also compare against the 175 billion parameter variant of GPT-3~\citep{NEURIPS2020_1457c0d6}, which utilizes in-context learning with the same number of few-shot examples as all other models.

We find that, on average, T0 models trained using our FLAD-based methods outperform all other methods, and to the best of our knowledge, our methods lead to the first 3 billion parameter models that outperform GPT-3 on this dataset mixture (previous smallest models have 11 billion parameters). Additionally, the T5-XL models trained with our FLAD-based methods are only outperformed by GPT-3, which has $62.5$ times more parameters. % Don't these two sentences contradict each other?
Additionally, we find that our FLAD-based methods provide robust performance across datasets, achieving best or second-best performance on $9/11$ datasets, only perfomorning worst on COPA.
